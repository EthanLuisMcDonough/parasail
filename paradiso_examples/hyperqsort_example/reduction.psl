import Node_Group::*;

interface Reduction
  <Accum_Type is Assignable<>;
   Combiner: func (var Left : Accum_Type; Right : Accum_Type);
   Identity : Accum_Type;
   Index_Type is Integer <>;
   Kind_Id : Distrib_Kind_Id_Type>
  extends Distrib_Obj_Handle is
   //  This provides a "Collector" type which is a distributed object
   //  spread over the nodes.  Ultimately, the "Distrib_Reduce"
   //  func takes an "Add_One_Value" func which adds a value
   //  to an accumulator that was initialized by copying from the initial
   //  state of the "Result" parameter.  The Add_One_Value func only
   //  gets two parameters, namely an accumulator where a value should be
   //  added, and an index which presumably identifies where to get the
   //  value.
   func Create_Collector
     (ref var GC : Node_Group) -> Reduction;

   func Distrib_Reduce
     (var Dis_Obj : Reduction;
      var Result : Accum_Type;
      First : Index_Type;
      Last : Index_Type_Base;
      Num_Chunks : Natural := 0;
      Add_One_Value : func
        (var Accum : Accum_Type; Index : Index_Type));
      //  Iterate in parallel over local indices 1 to Vec.Length, in chunks.
      //  Call Add_One_Value to add a single value to the accumulator.
      //  Will call Combiner to combine accumulators built up
      //  in parallel (some day doing a tree-wise combination in case
      //  the number of threads on a given node is very large).
      //  The node/shard-level result is shared with other nodes/shards
      //  and then combined into a single global accumulator, namely Result.

end interface Reduction;

class Reduction is

   const Debug_Distrib : Boolean := #false;

   type Reduction_Message_Kind is Enum<[Update_One_Accum]>;

   interface Reduction_Message_Type<> 
     extends Root_Message_Type is
      const Node_Value : Accum_Ptr;
   end interface Reduction_Message_Type

   func Handle_Obj_Message
     (Dis_Obj : in out Reduction_Info;
      Msg : aliased Ada.Streams.Stream_Element_Array);
   //  Message received that was directed to this distributed object

      func Handle_Obj_Message
        (Dis_Obj : in out Reduction_Info;
         Msg : aliased Ada.Streams.Stream_Element_Array) is

         use Buffered_Streams;
         Reader : aliased Buffered_Reader (Msg'Access);
         //  Read message out of stream
         Message : constant Reduction_Message_Type :=
           Reduction_Message_Type'Input (Reader'Access);

      begin

         if Debug_Distrib then
            Println (Dis_Obj.Self.GC.Node_Index'Image |
              ": message received: " | Message.Kind'Image);
         end if;

         //  Handle message
         case Message.Kind is
            when Update_One_Accum =>
               if Debug_Distrib then
                  Println (Dis_Obj.Self.GC.Node_Index'Image | ": obj " |
                    Distrib_Type_Id | Dis_Obj.Self.Distrib_Obj_Id'Image |
                    " received message from node =" |
                      Message.Node_Index'Image |
                    ", time stamp =" | Message.Time_Stamp'Image);
               end if;
               if Message.Node_Index /= Dis_Obj.Self.GC.Node_Index
                 and then Message.Time_Stamp >
                   Dis_Obj.Time_Stamps (Message.Node_Index)
               then
                  //  This message is not from current node,
                  //  and it is at a higher time stamp than past
                  //  update from this node, so update
                  //  the node info.
                  Dis_Obj.Accums (Message.Node_Index) := Message.Node_Value;
                  Dis_Obj.Time_Stamps (Message.Node_Index) :=
                    Message.Time_Stamp;
               else
                  if Debug_Distrib then
                     Println (Dis_Obj.Self.GC.Node_Index'Image |
                       ": message ignored");
                  end if;
               end if;
         end case;
      end Handle_Obj_Message;
   end Distrib;

   type Reduction_Info is limited new Distrib.Reduction_Info with null record;

   func Init_Info (Self : not null access Collector)
     -> not null Reduction_Info_Ptr is
   begin
      return Result : constant not null Reduction_Info_Ptr :=
        new Reduction_Info
          (Self, Self.GC.Num_Nodes, Node_Index => Self.GC.Node_Index) do
         if Self.Distrib_Obj_Id >= Next_Obj_Id_To_Assign then
            Next_Obj_Id_To_Assign := Self.Distrib_Obj_Id + 1;
         end if;
         Create_Distrib_Obj
           (Self.GC.all, Result.all'Unchecked_Access);
      end return;
   end Init_Info;

   func Next_Obj_Id() -> Distrib_Object_Id_Type is
      //  This can be used to assign the next sequential object id
      //  presuming the Distrib_Type_Id is unique to each instance
      //  of this package.
      Result : constant Distrib_Object_Id_Type := Next_Obj_Id_To_Assign;
   begin
      Next_Obj_Id_To_Assign := Next_Obj_Id_To_Assign + 1;
      return Result;
   end Next_Obj_Id;

   func Share_And_Wait
     (Dis_Obj : in out Collector; Accum : Accum_Type) is
   //  Share value of accum with other nodes and then wait for updates
      Time_Stamp : constant Time_Stamp_Type :=
         Dis_Obj.Info.Time_Stamps (Dis_Obj.GC.Node_Index) + 1;

      Message : constant Reduction_Message_Type :=
        (Kind => Update_One_Accum,
         Time_Stamp => Time_Stamp,
         Node_Index => Dis_Obj.GC.Node_Index,
         Node_Value => new Accum_Type'(Accum));
      Stream : aliased Buffered_Streams.Buffered_Stream;
   begin
      Dis_Obj.Info.Accums (Dis_Obj.GC.Node_Index) := Message.Node_Value;
      Dis_Obj.Info.Time_Stamps (Dis_Obj.GC.Node_Index) := Time_Stamp;
      Reduction_Message_Type'Write (Stream'Access, Message);

      System_Distrib.Share_With_Peers
        (Dis_Obj.GC.all, Dis_Obj.Info.all, Stream.Contents);
      if Debug_Distrib then
         Println ("Node " | Dis_Obj.GC.Node_Index'Image |
           " about to wait until Time stamp for obj " |
           Dis_Obj.Info.Distrib_Type_Id |
           Dis_Obj.Info.Distrib_Obj_Id'Image | " >=" |
           Time_Stamp'Image);
      end if;

      //  Keep receiving messages as long as some node has an old time stamp.
      while (for some I in Dis_Obj.Info.Time_Stamps'Range =>
                Dis_Obj.Info.Time_Stamps (I) < Time_Stamp)
      loop
         //  Wait for a message and then re-check
         Dis_Obj.Info.Handle_Queued_Messages (Wait_For_Message => True);
      end loop;
   end Share_And_Wait;

  exports

   func Distrib_Reduce
     (var Dis_Obj : Collector;
      var Result : Accum_Type;
      First : Index_Type;
      Last : Index_Type_Base;
      Num_Chunks : Natural := 0;
      Add_One_Value : func
        (var Accum : Accum_Type; Index : Index_Type)) is
      //  Iterate in parallel over local indices First to Last, in chunks.
      //  Call Add_One_Value to add a single value to the accumulator.
      //  Will call Combiner to combine accumulators built up
      //  in parallel (some day doing a tree-wise combination in case
      //  the number of threads on a given node is very large).
      //  The node/shard-level result is shared with other nodes/shards
      //  and then combined into a single global accumulator, namely Result.

      Accum_Identity : constant Accum_Type := Result;
         //  Accum initially has "identity" value.
         //  TBD: Might be better to pass in an Identity separately.

      Local_Accum : Accum_Type := Accum_Identity;
         //  Local accumulator for this shard

      protected Prot_Combiner is
         func Combine (New_Accum : in out Accum_Type);
      end Prot_Combiner;

      protected body Prot_Combiner is
         func Combine (New_Accum : in out Accum_Type) is
         begin
            Combiner (Left => Local_Accum, Right => New_Accum);
         end Combine;
      end Prot_Combiner;

      use System.Parallelism;

      func Reduce_Over_One_Chunk
        (Low, High : Longest_Integer; Chunk_Index : Positive) is
         Accum : Accum_Type := Accum_Identity;
      begin
         //  Iterate over the chunk, accumulating using Add_One_Value
         for I in Low .. High loop
            Add_One_Value (Accum, Index_Type (I));
         end loop;
         //  Now combine into the per-shard accumulator
         Prot_Combiner.Combine (Accum);
      end Reduce_Over_One_Chunk;

   begin  //  Distrib_Reduce

      Par_Range_Loop
        (Low => Longest_Integer (First), High => Longest_Integer (Last),
         Num_Chunks => Num_Chunks,
         Loop_Body => Reduce_Over_One_Chunk'Access);

      //  Share accumulator with other nodes
      Share_And_Wait (Dis_Obj, Local_Accum);

      //  Now combine the accumulators left to right into Result.
      for I in 1 .. Dis_Obj.GC.Num_Nodes loop
         Combiner (Left => Result, Right => Dis_Obj.Info.Accums (I).all);
      end loop;
   end Distrib_Reduce;

end class Reduction;
